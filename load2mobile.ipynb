{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models.quantization import resnet18 as resnet18q\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "from torch.quantization import fuse_modules\n",
    "from torch.utils.bundled_inputs import augment_model_with_bundled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PyTorch model\n",
    "model = resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Generate input image\n",
    "example = torch.zeros(10, 3, 224, 224)\n",
    "\n",
    "# Save model graph to TorchScript format\n",
    "torchscript_model = torch.jit.trace(model, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.torchvision.models.resnet.___torch_mangle_92.ResNet,\n",
      "      %input.1 : Float(10, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)):\n",
      "  %1140 : __torch__.torch.nn.modules.linear.___torch_mangle_91.Linear = prim::GetAttr[name=\"fc\"](%self.1)\n",
      "  %1137 : __torch__.torch.nn.modules.pooling.___torch_mangle_90.AdaptiveAvgPool2d = prim::GetAttr[name=\"avgpool\"](%self.1)\n",
      "  %1136 : __torch__.torch.nn.modules.container.___torch_mangle_89.Sequential = prim::GetAttr[name=\"layer4\"](%self.1)\n",
      "  %1095 : __torch__.torch.nn.modules.container.___torch_mangle_73.Sequential = prim::GetAttr[name=\"layer3\"](%self.1)\n",
      "  %1054 : __torch__.torch.nn.modules.container.___torch_mangle_57.Sequential = prim::GetAttr[name=\"layer2\"](%self.1)\n",
      "  %1013 : __torch__.torch.nn.modules.container.___torch_mangle_41.Sequential = prim::GetAttr[name=\"layer1\"](%self.1)\n",
      "  %980 : __torch__.torch.nn.modules.pooling.___torch_mangle_28.MaxPool2d = prim::GetAttr[name=\"maxpool\"](%self.1)\n",
      "  %979 : __torch__.torch.nn.modules.activation.___torch_mangle_27.ReLU = prim::GetAttr[name=\"relu\"](%self.1)\n",
      "  %978 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_26.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%self.1)\n",
      "  %973 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name=\"conv1\"](%self.1)\n",
      "  %1216 : Tensor = prim::CallMethod[name=\"forward\"](%973, %input.1)\n",
      "  %1217 : Tensor = prim::CallMethod[name=\"forward\"](%978, %1216)\n",
      "  %1218 : Tensor = prim::CallMethod[name=\"forward\"](%979, %1217)\n",
      "  %1219 : Tensor = prim::CallMethod[name=\"forward\"](%980, %1218)\n",
      "  %1220 : Tensor = prim::CallMethod[name=\"forward\"](%1013, %1219)\n",
      "  %1221 : Tensor = prim::CallMethod[name=\"forward\"](%1054, %1220)\n",
      "  %1222 : Tensor = prim::CallMethod[name=\"forward\"](%1095, %1221)\n",
      "  %1223 : Tensor = prim::CallMethod[name=\"forward\"](%1136, %1222)\n",
      "  %1224 : Tensor = prim::CallMethod[name=\"forward\"](%1137, %1223)\n",
      "  %890 : int = prim::Constant[value=1]() # /Users/fox/cloud/miniconda/envs/qals/lib/python3.7/site-packages/torchvision/models/resnet.py:243:0\n",
      "  %891 : int = prim::Constant[value=-1]() # /Users/fox/cloud/miniconda/envs/qals/lib/python3.7/site-packages/torchvision/models/resnet.py:243:0\n",
      "  %input : Float(10, 512, strides=[512, 1], requires_grad=1, device=cpu) = aten::flatten(%1224, %890, %891) # /Users/fox/cloud/miniconda/envs/qals/lib/python3.7/site-packages/torchvision/models/resnet.py:243:0\n",
      "  %1225 : Tensor = prim::CallMethod[name=\"forward\"](%1140, %input)\n",
      "  return (%1225)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torchscript_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor) -> Tensor:\n",
      "  _0 = self.fc\n",
      "  _1 = self.avgpool\n",
      "  _2 = self.layer4\n",
      "  _3 = self.layer3\n",
      "  _4 = self.layer2\n",
      "  _5 = self.layer1\n",
      "  _6 = self.maxpool\n",
      "  _7 = self.relu\n",
      "  _8 = (self.bn1).forward((self.conv1).forward(input, ), )\n",
      "  _9 = (_5).forward((_6).forward((_7).forward(_8, ), ), )\n",
      "  _10 = (_2).forward((_3).forward((_4).forward(_9, ), ), )\n",
      "  input0 = torch.flatten((_1).forward(_10, ), 1, -1)\n",
      "  return (_0).forward(input0, )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torchscript_model.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why TorchScript?\n",
    "\n",
    "- TorchScript code can be invoked in its own interpreter, which is basically a restricted Python interpreter. \n",
    "\n",
    "- This interpreter does not acquire the Global Interpreter Lock, and so many requests can be processed on the same instance simultaneously.\n",
    "\n",
    "- This format allows us to save the whole model to disk and load it into another environment, such as in a server written in a language other than Python\n",
    "\n",
    "- TorchScript gives us a representation in which we can do compiler optimizations on the code to provide more efficient execution\n",
    "\n",
    "- TorchScript allows us to interface with many backend/device runtimes that require a broader view of the program than individual operators.\n",
    "\n",
    "- We can see that invoking traced model produces the same results as the Python module\n",
    "\n",
    "Inroduction to TorchScript: https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input image\n",
    "example = torch.zeros(10, 3, 224, 224)\n",
    "\n",
    "# Load PyTorch model\n",
    "model = resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Fuse PyTorch model\n",
    "layers_to_fuse = [[\"conv1\", \"bn1\", \"relu\"]]\n",
    "for i in range(1, 5):\n",
    "    for j in range(2):\n",
    "        for k in range(1, 3):\n",
    "            layers_to_fuse += [[\n",
    "                f'layer{i}.{j}.conv{k}', f'layer{i}.{j}.bn{k}'\n",
    "            ]]\n",
    "\n",
    "fuse_modules(model, layers_to_fuse, inplace=True)\n",
    "\n",
    "# Save model graph to TorchScript format\n",
    "torchscript_model = torch.jit.script(model)\n",
    "\n",
    "# Optimize for mobile PyTorch operations that are supported by Android framework\n",
    "# If operations are not supported, they remain unchanged\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "\n",
    "# Save binary file with model on the computer (without input image example)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_orig.ptl\")\n",
    "\n",
    "# Create a joint input consisting of model and input image\n",
    "augment_model_with_bundled_inputs(torchscript_model_optimized, [(example, )])\n",
    "\n",
    "# Save binary file with model on the computer (with input image example)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_orig2.ptl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fox/cloud/miniconda/envs/qals/lib/python3.7/site-packages/torch/quantization/observer.py:123: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    }
   ],
   "source": [
    "# Load PyTorch model\n",
    "model = resnet18q(pretrained=True, quantize=True)\n",
    "model.eval()\n",
    "\n",
    "# Generate input image\n",
    "example = torch.zeros(10, 3, 224, 224)\n",
    "\n",
    "# Save model graph to TorchScript format\n",
    "torchscript_model = torch.jit.script(model)\n",
    "\n",
    "# Optimize for mobile PyTorch operations that are supported by Android framework\n",
    "# If operations are not supported, they remain unchanged\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "\n",
    "# Save binary file with model on the computer (without input image example)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_quan.ptl\")\n",
    "\n",
    "# Create a joint input consisting of model and input image\n",
    "augment_model_with_bundled_inputs(torchscript_model_optimized, [(example, )])\n",
    "\n",
    "# Save binary file with model on the computer (with input image example)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_quan2.ptl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_by_name(model, mname: str):\n",
    "    module = model\n",
    "    mname_list = mname.split('.')\n",
    "    for mname in mname_list:\n",
    "        module = module._modules[mname]\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def replace_layer_by_name(model, mname: str, layer):\n",
    "    module = model\n",
    "    mname_list = mname.split('.')\n",
    "    for mname in mname_list[:-1]:\n",
    "        module = module._modules[mname]\n",
    "\n",
    "    module._modules[mname_list[-1]] = layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_cp3_decomposition(conv, cr):\n",
    "\n",
    "    kernel_size = np.prod(conv.kernel_size)\n",
    "    rank = int(cr * (kernel_size * conv.in_channels * conv.out_channels) /\n",
    "               (kernel_size + conv.in_channels + conv.out_channels))\n",
    "\n",
    "    cp3_conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=conv.in_channels,\n",
    "                  out_channels=rank,\n",
    "                  kernel_size=(1, 1),\n",
    "                  bias=False),\n",
    "        nn.Conv2d(in_channels=rank,\n",
    "                  out_channels=rank,\n",
    "                  kernel_size=conv.kernel_size,\n",
    "                  groups=rank,\n",
    "                  stride=conv.stride,\n",
    "                  padding=conv.padding,\n",
    "                  dilation=conv.dilation,\n",
    "                  bias=False),\n",
    "        nn.Conv2d(in_channels=rank,\n",
    "                  out_channels=conv.out_channels,\n",
    "                  kernel_size=(1, 1),\n",
    "                  bias=conv.bias is not None))\n",
    "\n",
    "    return cp3_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose compression ratio\n",
    "cr = 0.5\n",
    "\n",
    "# Load PyTorch model\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "\n",
    "# Select convolutional layers\n",
    "conv_layers = [\n",
    "    module_name for module_name, module in model.named_modules()\n",
    "    if isinstance(module, nn.Conv2d) and \"downsample\" not in module_name\n",
    "]\n",
    "\n",
    "# Compress model\n",
    "for layer_name in conv_layers:\n",
    "    layer = get_layer_by_name(model, layer_name)\n",
    "    cp3_decomposition = get_cp3_decomposition(layer, cr)\n",
    "    replace_layer_by_name(model, layer_name, cp3_decomposition)\n",
    "\n",
    "# Fuse model\n",
    "layers_to_fuse = [[\"conv1.2\", \"bn1\", \"relu\"]]\n",
    "for i in range(1, 5):\n",
    "    for j in range(2):\n",
    "        for k in range(1, 3):\n",
    "            layers_to_fuse += [[\n",
    "                f'layer{i}.{j}.conv{k}.2', f'layer{i}.{j}.bn{k}'\n",
    "            ]]\n",
    "\n",
    "model.eval()\n",
    "fuse_modules(model, layers_to_fuse, inplace=True)\n",
    "\n",
    "# Save model\n",
    "torchscript_model = torch.jit.script(model)\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_comp.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose compression ratio\n",
    "cr = 0.5\n",
    "\n",
    "# Load PyTorch model\n",
    "model = resnet18q(pretrained=True, quantize=False)\n",
    "\n",
    "# Select convolutional layers\n",
    "conv_layers = [\n",
    "    module_name for module_name, module in model.named_modules()\n",
    "    if isinstance(module, nn.Conv2d) and \"downsample\" not in module_name\n",
    "]\n",
    "\n",
    "# Compress model\n",
    "for layer_name in conv_layers:\n",
    "    layer = get_layer_by_name(model, layer_name)\n",
    "    cp3_decomposition = get_cp3_decomposition(layer, cr)\n",
    "    replace_layer_by_name(model, layer_name, cp3_decomposition)\n",
    "\n",
    "# # Fuse model\n",
    "# layers_to_fuse = [[\"conv1.2\", \"bn1\", \"relu\"]]\n",
    "# for i in range(1, 5):\n",
    "#     for j in range(2):\n",
    "#         for k in range(1, 3):\n",
    "#             layers_to_fuse += [[\n",
    "#                 f'layer{i}.{j}.conv{k}.2', f'layer{i}.{j}.bn{k}'\n",
    "#             ]]\n",
    "\n",
    "# model.eval()\n",
    "# fuse_modules(model, layers_to_fuse, inplace=True)\n",
    "\n",
    "# Quantize model\n",
    "model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "# Calibrate your model\n",
    "# def calibrate(model, calibration_data):\n",
    "#     # Your calibration code here\n",
    "#     return\n",
    "# calibrate(model, [])\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "# Save model\n",
    "torchscript_model = torch.jit.script(model)\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "augment_model_with_bundled_inputs(torchscript_model_optimized, [(example, )])\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\n",
    "    \"resnet18_comp_quan.ptl\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adb push speed_benchmark_torch_x86 /data/local/tmp\n",
    "\n",
    "adb push resnet18_orig.ptl       /data/local/tmp\n",
    "adb push resnet18_orig2.ptl      /data/local/tmp\n",
    "adb push resnet18_quan.ptl       /data/local/tmp\n",
    "adb push resnet18_quan2.ptl      /data/local/tmp\n",
    "adb push resnet18_comp.ptl       /data/local/tmp\n",
    "adb push resnet18_comp_quan.ptl  /data/local/tmp\n",
    "\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_orig.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_quan.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_comp.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_comp_quan.ptl\"  --iter=200 --warmup=100\n",
    "\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --use_bundled_input=0 --model=/data/local/tmp/resnet18_orig2.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --use_bundled_input=0 --model=/data/local/tmp/resnet18_quan2.ptl\"  --iter=200 --warmup=100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adb push speed_benchmark_torch_arm64-v8a /data/local/tmp\n",
    "\n",
    "adb push resnet18_orig.ptl       /data/local/tmp\n",
    "adb push resnet18_quan.ptl       /data/local/tmp\n",
    "adb push resnet18_comp.ptl       /data/local/tmp\n",
    "adb push resnet18_comp_quan.ptl  /data/local/tmp\n",
    "\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_arm64-v8a --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_orig.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_arm64-v8a --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_quan.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_arm64-v8a --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_comp.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_arm64-v8a --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_comp_quan.ptl\"  --iter=200 --warmup=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv resnet18*.plt app/src/main/assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 89.17584037780762\n",
      "Size (MB): 22.5590877532959\n",
      "Size (MB): 47.225582122802734\n",
      "Size (MB): 12.061223983764648\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    " print('Size (MB):', os.path.getsize(\"resnet18_orig.ptl\") / 1024**2)\n",
    " print('Size (MB):', os.path.getsize(\"resnet18_quan.ptl\") / 1024**2)\n",
    " print('Size (MB):', os.path.getsize(\"resnet18_comp.ptl\") / 1024**2)\n",
    " print('Size (MB):', os.path.getsize(\"resnet18_comp_quan.ptl\") / 1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnotatedConvBnReLUModel(\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "\n",
    "class AnnotatedConvBnReLUModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AnnotatedConvBnReLUModel, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(3, 5, 3, bias=False).to(dtype=torch.float)\n",
    "        # self.bn = torch.nn.BatchNorm2d(5).to(dtype=torch.float)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         x = x.contiguous(memory_format=torch.channels_last)\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        # x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AnnotatedConvBnReLUModel()\n",
    "torch.quantization.fuse_modules(model, [['conv', 'relu']], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnotatedConvBnReLUModel(\n",
       "  (conv): QuantizedConvReLU2d(3, 5, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
       "  (relu): Identity()\n",
       "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm') #'qnnpack') 'fbgemm'\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "# Calibrate your model\n",
    "# def calibrate(model, calibration_data):\n",
    "#     # Your calibration code here\n",
    "#     return\n",
    "# calibrate(model, [])\n",
    "torch.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchscript_model = torch.jit.script(model)\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"model.ptl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "617fd164454d89a79bc845549af1ea31c5c5e3f51cef245c47c24519c7d1b14a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('qals')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
