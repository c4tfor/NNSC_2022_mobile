{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models.quantization import resnet18 as resnet18q\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "from torch.quantization import fuse_modules\n",
    "from torch.utils.bundled_inputs import augment_model_with_bundled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input image\n",
    "example = torch.zeros(10, 3, 224, 224)\n",
    "\n",
    "# Load PyTorch model\n",
    "model = resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Fuse PyTorch model\n",
    "layers_to_fuse = [[\"conv1\", \"bn1\", \"relu\"]]\n",
    "for i in range(1, 5):\n",
    "    for j in range(2):\n",
    "        for k in range(1, 3):\n",
    "            layers_to_fuse += [[\n",
    "                f'layer{i}.{j}.conv{k}', f'layer{i}.{j}.bn{k}'\n",
    "            ]]\n",
    "\n",
    "fuse_modules(model, layers_to_fuse, inplace=True)\n",
    "\n",
    "# Save model graph to TorchScript format\n",
    "torchscript_model = torch.jit.script(model)\n",
    "\n",
    "# Optimize for mobile PyTorch operations that are supported by Android framework\n",
    "# If operations are not supported, they remain unchanged\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "\n",
    "# Save binary file with model on the computer (without input image example)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_orig.ptl\")\n",
    "\n",
    "# Create a joint input consisting of model and input image\n",
    "augment_model_with_bundled_inputs(torchscript_model_optimized, [(example, )])\n",
    "\n",
    "# Save binary file with model on the computer (with input image example)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_orig2.ptl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fox/cloud/miniconda/envs/qals/lib/python3.7/site-packages/torch/quantization/observer.py:123: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    }
   ],
   "source": [
    "# Load PyTorch model\n",
    "model = resnet18q(pretrained=True, quantize=True)\n",
    "model.eval()\n",
    "\n",
    "# Generate input image\n",
    "example = torch.zeros(10, 3, 224, 224)\n",
    "\n",
    "# Save model graph to TorchScript format\n",
    "torchscript_model = torch.jit.script(model)\n",
    "\n",
    "# Optimize for mobile PyTorch operations that are supported by Android framework\n",
    "# If operations are not supported, they remain unchanged\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "\n",
    "# Save binary file with model on the computer (without input image example)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_quan.ptl\")\n",
    "\n",
    "# Create a joint input consisting of model and input image\n",
    "augment_model_with_bundled_inputs(torchscript_model_optimized, [(example, )])\n",
    "\n",
    "# Save binary file with model on the computer (with input image example)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_quan2.ptl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_by_name(model, mname: str):\n",
    "    module = model\n",
    "    mname_list = mname.split('.')\n",
    "    for mname in mname_list:\n",
    "        module = module._modules[mname]\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def replace_layer_by_name(model, mname: str, layer):\n",
    "    module = model\n",
    "    mname_list = mname.split('.')\n",
    "    for mname in mname_list[:-1]:\n",
    "        module = module._modules[mname]\n",
    "\n",
    "    module._modules[mname_list[-1]] = layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_cp3_decomposition(conv, cr):\n",
    "\n",
    "    kernel_size = np.prod(conv.kernel_size)\n",
    "    rank = int(cr * (kernel_size * conv.in_channels * conv.out_channels) /\n",
    "               (kernel_size + conv.in_channels + conv.out_channels))\n",
    "\n",
    "    cp3_conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=conv.in_channels,\n",
    "                  out_channels=rank,\n",
    "                  kernel_size=(1, 1),\n",
    "                  bias=False),\n",
    "        nn.Conv2d(in_channels=rank,\n",
    "                  out_channels=rank,\n",
    "                  kernel_size=conv.kernel_size,\n",
    "                  groups=rank,\n",
    "                  stride=conv.stride,\n",
    "                  padding=conv.padding,\n",
    "                  dilation=conv.dilation,\n",
    "                  bias=False),\n",
    "        nn.Conv2d(in_channels=rank,\n",
    "                  out_channels=conv.out_channels,\n",
    "                  kernel_size=(1, 1),\n",
    "                  bias=conv.bias is not None))\n",
    "\n",
    "    return cp3_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose compression ratio\n",
    "cr = 0.5\n",
    "\n",
    "# Load PyTorch model\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "\n",
    "# Select convolutional layers\n",
    "conv_layers = [\n",
    "    module_name for module_name, module in model.named_modules()\n",
    "    if isinstance(module, nn.Conv2d) and \"downsample\" not in module_name\n",
    "]\n",
    "\n",
    "# Compress model\n",
    "for layer_name in conv_layers:\n",
    "    layer = get_layer_by_name(model, layer_name)\n",
    "    cp3_decomposition = get_cp3_decomposition(layer, cr)\n",
    "    replace_layer_by_name(model, layer_name, cp3_decomposition)\n",
    "\n",
    "# Fuse model\n",
    "layers_to_fuse = [[\"conv1.2\", \"bn1\", \"relu\"]]\n",
    "for i in range(1, 5):\n",
    "    for j in range(2):\n",
    "        for k in range(1, 3):\n",
    "            layers_to_fuse += [[\n",
    "                f'layer{i}.{j}.conv{k}.2', f'layer{i}.{j}.bn{k}'\n",
    "            ]]\n",
    "\n",
    "model.eval()\n",
    "fuse_modules(model, layers_to_fuse, inplace=True)\n",
    "\n",
    "# Save model\n",
    "torchscript_model = torch.jit.script(model)\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"resnet18_comp.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose compression ratio\n",
    "cr = 0.5\n",
    "\n",
    "# Load PyTorch model\n",
    "model = resnet18q(pretrained=True, quantize=False)\n",
    "\n",
    "# Select convolutional layers\n",
    "conv_layers = [\n",
    "    module_name for module_name, module in model.named_modules()\n",
    "    if isinstance(module, nn.Conv2d) and \"downsample\" not in module_name\n",
    "]\n",
    "\n",
    "# Compress model\n",
    "for layer_name in conv_layers:\n",
    "    layer = get_layer_by_name(model, layer_name)\n",
    "    cp3_decomposition = get_cp3_decomposition(layer, cr)\n",
    "    replace_layer_by_name(model, layer_name, cp3_decomposition)\n",
    "\n",
    "# # Fuse model\n",
    "# layers_to_fuse = [[\"conv1.2\", \"bn1\", \"relu\"]]\n",
    "# for i in range(1, 5):\n",
    "#     for j in range(2):\n",
    "#         for k in range(1, 3):\n",
    "#             layers_to_fuse += [[\n",
    "#                 f'layer{i}.{j}.conv{k}.2', f'layer{i}.{j}.bn{k}'\n",
    "#             ]]\n",
    "\n",
    "# model.eval()\n",
    "# fuse_modules(model, layers_to_fuse, inplace=True)\n",
    "\n",
    "# Quantize model\n",
    "model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "# Calibrate your model\n",
    "# def calibrate(model, calibration_data):\n",
    "#     # Your calibration code here\n",
    "#     return\n",
    "# calibrate(model, [])\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "# Save model\n",
    "torchscript_model = torch.jit.script(model)\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "augment_model_with_bundled_inputs(torchscript_model_optimized, [(example, )])\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\n",
    "    \"resnet18_comp_quan.ptl\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adb push speed_benchmark_torch_x86 /data/local/tmp\n",
    "\n",
    "adb push resnet18_orig.ptl       /data/local/tmp\n",
    "adb push resnet18_orig2.ptl      /data/local/tmp\n",
    "adb push resnet18_quan.ptl       /data/local/tmp\n",
    "adb push resnet18_quan2.ptl      /data/local/tmp\n",
    "adb push resnet18_comp.ptl       /data/local/tmp\n",
    "adb push resnet18_comp_quan.ptl  /data/local/tmp\n",
    "\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_orig.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_quan.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_comp.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_comp_quan.ptl\"  --iter=200 --warmup=100\n",
    "\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --use_bundled_input=0 --model=/data/local/tmp/resnet18_orig2.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_x86 --use_bundled_input=0 --model=/data/local/tmp/resnet18_quan2.ptl\"  --iter=200 --warmup=100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adb push speed_benchmark_torch_arm64-v8a /data/local/tmp\n",
    "\n",
    "adb push resnet18_orig.ptl       /data/local/tmp\n",
    "adb push resnet18_quan.ptl       /data/local/tmp\n",
    "adb push resnet18_comp.ptl       /data/local/tmp\n",
    "adb push resnet18_comp_quan.ptl  /data/local/tmp\n",
    "\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_arm64-v8a --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_orig.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_arm64-v8a --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_quan.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_arm64-v8a --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_comp.ptl\"  --iter=200 --warmup=100\n",
    "adb shell \"/data/local/tmp/speed_benchmark_torch_arm64-v8a --input_dims=\"1,3,224,224\" --input_type=\"float\" --model=/data/local/tmp/resnet18_comp_quan.ptl\"  --iter=200 --warmup=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv resnet18*.plt app/src/main/assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 89.17584037780762\n",
      "Size (MB): 22.5590877532959\n",
      "Size (MB): 47.225582122802734\n",
      "Size (MB): 12.061223983764648\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    " print('Size (MB):', os.path.getsize(\"resnet18_orig.ptl\") / 1024**2)\n",
    " print('Size (MB):', os.path.getsize(\"resnet18_quan.ptl\") / 1024**2)\n",
    " print('Size (MB):', os.path.getsize(\"resnet18_comp.ptl\") / 1024**2)\n",
    " print('Size (MB):', os.path.getsize(\"resnet18_comp_quan.ptl\") / 1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnotatedConvBnReLUModel(\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "\n",
    "class AnnotatedConvBnReLUModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AnnotatedConvBnReLUModel, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(3, 5, 3, bias=False).to(dtype=torch.float)\n",
    "        # self.bn = torch.nn.BatchNorm2d(5).to(dtype=torch.float)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         x = x.contiguous(memory_format=torch.channels_last)\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        # x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AnnotatedConvBnReLUModel()\n",
    "torch.quantization.fuse_modules(model, [['conv', 'relu']], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnotatedConvBnReLUModel(\n",
       "  (conv): QuantizedConvReLU2d(3, 5, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, bias=False)\n",
       "  (relu): Identity()\n",
       "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm') #'qnnpack') 'fbgemm'\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "# Calibrate your model\n",
    "# def calibrate(model, calibration_data):\n",
    "#     # Your calibration code here\n",
    "#     return\n",
    "# calibrate(model, [])\n",
    "torch.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchscript_model = torch.jit.script(model)\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "torchscript_model_optimized._save_for_lite_interpreter(\"model.ptl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "617fd164454d89a79bc845549af1ea31c5c5e3f51cef245c47c24519c7d1b14a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('qals')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
